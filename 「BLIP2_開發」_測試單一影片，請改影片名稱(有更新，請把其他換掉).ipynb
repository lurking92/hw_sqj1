{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lurking92/hw_sqj1/blob/main/%E3%80%8CBLIP2_%E9%96%8B%E7%99%BC%E3%80%8D_%E6%B8%AC%E8%A9%A6%E5%96%AE%E4%B8%80%E5%BD%B1%E7%89%87%EF%BC%8C%E8%AB%8B%E6%94%B9%E5%BD%B1%E7%89%87%E5%90%8D%E7%A8%B1(%E6%9C%89%E6%9B%B4%E6%96%B0%EF%BC%8C%E8%AB%8B%E6%8A%8A%E5%85%B6%E4%BB%96%E6%8F%9B%E6%8E%89).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0EoNXzedJO8",
        "outputId": "1c26aab1-4735-4999-de64-0391f4295937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "torch>=2.0.0\n",
        "transformers>=4.37.0\n",
        "accelerate>=0.24.1\n",
        "Pillow>=10.0.0\n",
        "tqdm\n",
        "opencv-python\n",
        "ffmpeg-python\n",
        "sentencepiece\n",
        "sacremoses\n",
        "mediapipe\n",
        "scipy\n",
        "numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls7BvNSSZ7oS",
        "outputId": "cb9589be-c67d-4b92-a835-fd4f1ac078a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.53.1)\n",
            "Requirement already satisfied: accelerate>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: Pillow>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (11.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.11.0.86)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.1.1)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.10.21)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (0.33.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.24.1->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses->-r requirements.txt (line 9)) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses->-r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 10)) (0.5.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.37.0->-r requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 10)) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe->-r requirements.txt (line 10)) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe->-r requirements.txt (line 10)) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 10)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.37.0->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.37.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.37.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.37.0->-r requirements.txt (line 2)) (2025.6.15)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 10)) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe->-r requirements.txt (line 10)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 安裝依賴\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aa36GU9Pa-1Q"
      },
      "outputs": [],
      "source": [
        "# 建立資料夾\n",
        "!mkdir -p data/videos data/frames outputs scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Ux7v8_zc3P",
        "outputId": "8ab9bd8d-a955-4367-8aeb-61206c4f8412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "5GLAFj2NbAcu",
        "outputId": "5555dffe-b808-4cae-a1b7-e6f355a0b34c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de71910b-c798-4c29-bdae-7589a4931c2d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de71910b-c798-4c29-bdae-7589a4931c2d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving my_test_video.mp4 to my_test_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# 上傳影片（可選）\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import shutil\n",
        "import os\n",
        "for fname in uploaded.keys():\n",
        "    os.makedirs(\"data/videos\", exist_ok=True)\n",
        "    shutil.move(fname, f\"data/videos/{fname}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xdlRsi1bCAF",
        "outputId": "46de3170-0b39-4caa-cf2c-78dfaa997cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/extract_frames.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/extract_frames.py\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def extract_frames(video_path, output_dir, interval_sec=2):\n",
        "    os.makedirs(output_dir, exist_ok=True) # 確保輸出資料夾存在\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", video_path,\n",
        "        \"-vf\", f\"fps=1/{interval_sec}\",\n",
        "        os.path.join(output_dir, \"frame_%03d.jpg\") # 這裡會直接寫入 output_dir\n",
        "    ]\n",
        "    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwnZHU_ZcAXy",
        "outputId": "d8406702-e5ea-4d44-8a91-6de8abfe70d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/detect_objects.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/detect_objects.py\n",
        "def detect_objects(image_path):\n",
        "# 模擬用，實際應用時請用 GroundingDINO 推論\n",
        "    return [\"person\", \"chair\", \"table\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YFFYmelcCgw",
        "outputId": "564b0d9b-7c67-4cb4-c845-af502b4dd1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/segment_objects.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/segment_objects.py\n",
        "def segment_objects(image_path, object_list):\n",
        "# 模擬回傳：實際應該回傳分割後圖像（或原圖）\n",
        "# 為簡化處理，我們先用原始圖像重複多份\n",
        "    return [image_path] * len(object_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIkn_9PEZYFo",
        "outputId": "a5b0cba8-5791-480d-880d-bb104acdfd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/analyze_pose.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/analyze_pose.py\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=True)\n",
        "\n",
        "# 優先動作排序\n",
        "ACTION_PRIORITY = [\n",
        "    \"跌倒\", \"坐下\", \"撿起物品\", \"放下物品\", \"拿起物品\", \"打開東西\", \"關閉東西\",\n",
        "    \"走路\", \"轉身\", \"查看\", \"觸碰\", \"站立\", \"等待\"\n",
        "]\n",
        "\n",
        "def classify_pose(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        return \"未知\"\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    result = pose.process(image_rgb)\n",
        "\n",
        "    if not result.pose_landmarks:\n",
        "        return \"無法辨識\"\n",
        "\n",
        "    lm = result.pose_landmarks.landmark\n",
        "\n",
        "    nose_y = lm[mp_pose.PoseLandmark.NOSE].y\n",
        "    left_knee_y = lm[mp_pose.PoseLandmark.LEFT_KNEE].y\n",
        "    right_knee_y = lm[mp_pose.PoseLandmark.RIGHT_KNEE].y\n",
        "    left_ankle_y = lm[mp_pose.PoseLandmark.LEFT_ANKLE].y\n",
        "    right_ankle_y = lm[mp_pose.PoseLandmark.RIGHT_ANKLE].y\n",
        "\n",
        "    avg_knee_y = (left_knee_y + right_knee_y) / 2\n",
        "    avg_ankle_y = (left_ankle_y + right_ankle_y) / 2\n",
        "\n",
        "    vertical_span = avg_ankle_y - nose_y\n",
        "\n",
        "    if vertical_span < 0.4:\n",
        "        return \"跌倒\"\n",
        "    elif nose_y - avg_knee_y < 0.1:\n",
        "        return \"坐下\"\n",
        "    elif 0.1 < vertical_span < 0.35:\n",
        "        return \"走路\"\n",
        "    else:\n",
        "        return \"站立\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUxwvDaJcExA",
        "outputId": "4ddefe53-e8c2-4541-eef5-c867eaba3207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/generate_caption.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/generate_caption.py\n",
        "# scripts/generate_caption.py\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, pipeline\n",
        "from PIL import Image\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "\n",
        "# 使用 BLIP2 OPT 6.7B 模型，這個版本在速度和效能之間有很好的平衡\n",
        "# 請注意，這裡我們不再使用 FLAN-T5 XL 版本\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 翻譯模型保持不變\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\", src_lang=\"en\", tgt_lang=\"zh\")\n",
        "\n",
        "STOPWORDS = [\"gta\", \"截圖\", \"screenshot\", \"undefined\", \"圖片\", \"畫面\", \"gtp\"]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[\\-\\s]*gta[\\-\\s]*\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"[\\-\\s]*gtp[\\-\\s]*\", \"\", text, flags=re.IGNORECASE)\n",
        "    # 修正正則表達式，確保能正確處理連續重複的詞彙，例如 \"人, 人\" 變成 \"人\"\n",
        "    text = re.sub(r\"([,\\s]*)([\\u4e00-\\u9fa5a-zA-Z0-9]+)(,\\s*\\2)+\", r\"\\1\\2\", text)\n",
        "    for w in STOPWORDS:\n",
        "        text = text.replace(w, \"\")\n",
        "    return text.strip(\" ,\")\n",
        "\n",
        "def is_valid_english_caption(caption):\n",
        "    if not caption:\n",
        "        return False\n",
        "    # 增加檢查，避免非常短且無意義的內容，例如只有一個詞或重複詞\n",
        "    if len(set(caption.lower().split())) <= 2 and len(caption.split()) < 5:\n",
        "        return False\n",
        "    if re.match(r\"^[,~.\\se]+$\", caption): # 只包含標點符號或空白的字串\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def score_caption(caption):\n",
        "    score = len(caption)\n",
        "    # 提高與人物、動作、物品相關詞彙的分數權重\n",
        "    if any(word in caption.lower() for word in [\"person\", \"man\", \"woman\", \"walking\", \"falling\", \"picking\", \"holding\", \"entering\", \"sitting\", \"standing\"]):\n",
        "        score += 30 # 提高權重\n",
        "    if any(word in caption.lower() for word in [\"table\", \"chair\", \"object\", \"item\"]):\n",
        "        score += 10 # 物品也給予分數\n",
        "    return score\n",
        "\n",
        "def generate_caption(image_paths, context_hint=\"Describe this image.\", pose_label=None, debug_log_path=\"outputs/debug.log\"):\n",
        "    best_caption = \"\"\n",
        "    max_score = -1\n",
        "\n",
        "    hint = \"Describe what the person is doing and what objects are involved. Focus on movement, interaction, and location.\"\n",
        "    if pose_label:\n",
        "        # 將姿勢資訊更直接地整合到提示中，引導模型關注相關動作\n",
        "        hint = f\"The person is performing the action: {pose_label}. Specifically, {hint}\"\n",
        "\n",
        "    os.makedirs(os.path.dirname(debug_log_path), exist_ok=True)\n",
        "    debug_log = open(debug_log_path, \"a\", encoding=\"utf-8\")\n",
        "\n",
        "    for path in image_paths:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        captions = []\n",
        "        # 增加 num_return_sequences 以生成更多候選字幕進行評分，提高選到好字幕的機會\n",
        "        # 同時調整 num_beams 和 max_new_tokens 可能會影響速度和品質的平衡\n",
        "        for _ in range(3): # 生成多輪，每輪產生多個候選\n",
        "            inputs = processor(images=image, text=hint, return_tensors=\"pt\").to(model.device, dtype=next(model.parameters()).dtype)\n",
        "            out = model.generate(**inputs, max_new_tokens=50, num_beams=5, early_stopping=True, num_return_sequences=3)\n",
        "            for seq in out:\n",
        "                caption = processor.tokenizer.decode(seq, skip_special_tokens=True).strip()\n",
        "                captions.append(caption)\n",
        "\n",
        "        best_local = \"\"\n",
        "        local_max = -1\n",
        "        for c in captions:\n",
        "            if is_valid_english_caption(c):\n",
        "                s = score_caption(c)\n",
        "                if s > local_max:\n",
        "                    best_local = c\n",
        "                    local_max = s\n",
        "\n",
        "        debug_log.write(f\"Image: {path}\\n\")\n",
        "        for c in captions:\n",
        "            debug_log.write(f\"  Raw: {c}\\n\")\n",
        "        debug_log.write(f\"  Selected: {best_local}\\n\")\n",
        "\n",
        "        if local_max > max_score:\n",
        "            best_caption = best_local\n",
        "            max_score = local_max\n",
        "\n",
        "    debug_log.write(f\"Best caption selected overall: {best_caption}\\n\\n\")\n",
        "    debug_log.close()\n",
        "\n",
        "    if not best_caption:\n",
        "        zh = \"畫面中無法辨識明確內容\"\n",
        "    else:\n",
        "        zh = translator(best_caption)[0]['translation_text']\n",
        "        zh = clean_text(zh) # 確保翻譯後的中文也進行清理\n",
        "\n",
        "    if pose_label:\n",
        "        # 將動作標籤放在括號內，並確保前後有空格，避免與中文內容黏在一起\n",
        "        return f\"{zh}（動作：{pose_label}）\"\n",
        "    return zh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGVfpXMdcGkV",
        "outputId": "82cef43e-08c7-4acb-bb18-d7a5feb48ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/write_srt.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/write_srt.py\n",
        "def seconds_to_srt_time(seconds):\n",
        "  hrs = seconds // 3600\n",
        "  mins = (seconds % 3600) // 60\n",
        "  secs = seconds % 60\n",
        "  ms = int((seconds - int(seconds)) * 1000)\n",
        "  return f\"{int(hrs):02}:{int(mins):02}:{int(secs):02},{ms:03}\"\n",
        "\n",
        "def write_srt(captions, output_path):\n",
        "  with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for i, (start, end, text) in enumerate(captions, start=1):\n",
        "      f.write(f\"{i}\\n\")\n",
        "      f.write(f\"{seconds_to_srt_time(start)} --> {seconds_to_srt_time(end)}\\n\")\n",
        "      f.write(f\"{text}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwEUNroBcJfP",
        "outputId": "186c930a-2915-4fab-a339-deddd13f8035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "# 專案主程式 main.py\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import json # 導入 json 模組來儲存進度\n",
        "from tqdm.auto import tqdm\n",
        "from scripts.extract_frames import extract_frames\n",
        "from scripts.detect_objects import detect_objects\n",
        "from scripts.segment_objects import segment_objects\n",
        "from scripts.generate_caption import generate_caption\n",
        "from scripts.write_srt import write_srt\n",
        "from scripts.analyze_pose import classify_pose\n",
        "\n",
        "VIDEO_GLOB = 'data/videos/my_test_video.mp4' # 或者 'data/*/scene*/**/*.mp4'\n",
        "\n",
        "DRIVE_BASE_DIR = '/content/drive/MyDrive/Colab_Subtitles_Output' # Google Drive 上的基礎路徑\n",
        "FRAME_DIR = os.path.join(DRIVE_BASE_DIR, 'frames') # 圖片幀將會儲存在這裡\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE_DIR, 'outputs') # SRT 字幕會儲存在這裡\n",
        "\n",
        "# 確保 Google Drive 中的基礎資料夾和子資料夾存在\n",
        "os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "os.makedirs(FRAME_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 定義進度檔案的路徑\n",
        "PROGRESS_FILE = os.path.join(DRIVE_BASE_DIR, 'processing_progress.json')\n",
        "\n",
        "def save_progress(video_name, current_index):\n",
        "    \"\"\"儲存每個影片的處理進度\"\"\"\n",
        "    progress_data = {}\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                progress_data = json.load(f)\n",
        "            except json.JSONDecodeError: # 防止文件損壞\n",
        "                progress_data = {}\n",
        "\n",
        "    progress_data[video_name] = current_index\n",
        "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(progress_data, f, indent=4)\n",
        "\n",
        "def load_progress(video_name):\n",
        "    \"\"\"載入特定影片的處理進度\"\"\"\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                progress_data = json.load(f)\n",
        "                return progress_data.get(video_name, 0)\n",
        "            except json.JSONDecodeError:\n",
        "                return 0 # 文件損壞則從頭開始\n",
        "    return 0\n",
        "\n",
        "# --- 主要處理邏輯開始 ---\n",
        "video_paths = glob.glob(VIDEO_GLOB, recursive=True)\n",
        "\n",
        "if not video_paths:\n",
        "    print(f\"警告：未找到符合 '{VIDEO_GLOB}' 模式的影片。請檢查影片路徑或上傳。\")\n",
        "\n",
        "for video_path in video_paths:\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    frame_subdir = os.path.join(FRAME_DIR, video_name) # 每個影片的幀有自己的子資料夾\n",
        "\n",
        "    # ====== 檢查是否已存在圖片幀 ======\n",
        "    if os.path.exists(frame_subdir) and len(glob.glob(os.path.join(frame_subdir, \"*.jpg\"))) > 0:\n",
        "        print(f\"\\n[1] 影片 {video_name} 的圖片幀已存在於 Google Drive ({frame_subdir})，跳過提取。\")\n",
        "    else:\n",
        "        os.makedirs(frame_subdir, exist_ok=True)\n",
        "        print(f\"\\n[1] 處理影片: {video_path} → 擷取圖片中，儲存至 Google Drive ({frame_subdir})...\")\n",
        "        extract_frames(video_path, frame_subdir, interval_sec=2)\n",
        "    # =======================================\n",
        "\n",
        "    print(f\"[2] 處理 {video_name} 的每一幀...\")\n",
        "    captions = []\n",
        "    image_files = sorted(os.listdir(frame_subdir))\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"警告：影片 {video_name} 未擷取到任何圖片幀，跳過字幕生成。\")\n",
        "        continue\n",
        "\n",
        "    total_frames = len(image_files)\n",
        "    start_index = load_progress(video_name) # 載入上次的進度\n",
        "\n",
        "    if start_index > 0:\n",
        "        print(f\"從上次進度繼續：影片 {video_name} 已處理 {start_index}/{total_frames} 幀。\")\n",
        "        # 如果是從中間開始，需要重新構建已經處理部分的字幕列表 (這是一個簡化，假設你不需要斷點續寫SRT)\n",
        "        # 由於我們在迴圈結束後才寫入 SRT，這裡可以假設從斷點開始收集即可。\n",
        "        # 如果你希望更精確到每次重啟都只處理未完成的，那這裡需要修改 captions 的初始化\n",
        "        # 最簡單是直接忽略斷點前的字幕，因為最終會生成一個完整的srt\n",
        "        pass\n",
        "\n",
        "    # 使用 tqdm 包裹 image_files 迴圈，顯示進度條，並從 start_index 開始\n",
        "    # tqdm 的 initial 參數可以設定進度條的起始值\n",
        "    for i in tqdm(range(start_index, total_frames), initial=start_index, total=total_frames, desc=f\"生成 {video_name} 的字幕\"):\n",
        "        fname = image_files[i]\n",
        "        image_path = os.path.join(frame_subdir, fname)\n",
        "\n",
        "        # 檢查圖片檔案是否存在\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"警告：圖片文件 {image_path} 不存在，跳過該幀。\")\n",
        "            save_progress(video_name, i + 1) # 即使跳過也更新進度\n",
        "            continue\n",
        "\n",
        "        pose_label = classify_pose(image_path)\n",
        "        objects = detect_objects(image_path)\n",
        "        focused_images = segment_objects(image_path, objects)\n",
        "\n",
        "        zh_caption = generate_caption(\n",
        "            focused_images,\n",
        "            context_hint=\"Describe what the person is doing, where they are, and what objects are present. Use natural language.\",\n",
        "            pose_label=pose_label\n",
        "        )\n",
        "        start_time = i * 2\n",
        "        end_time = (i + 1) * 2\n",
        "        captions.append((start_time, end_time, zh_caption))\n",
        "\n",
        "        # ====== 新增：每處理完一幀就儲存進度 ======\n",
        "        save_progress(video_name, i + 1)\n",
        "        # =======================================\n",
        "\n",
        "    # 在迴圈外部寫入 SRT 檔案，確保包含所有幀的字幕\n",
        "    srt_path = os.path.join(OUTPUT_DIR, f\"{video_name}.srt\")\n",
        "    print(f\"[3] 輸出字幕檔至 {srt_path} (已保存到 Google Drive)...\")\n",
        "\n",
        "    # 這裡的 captions 列表只包含從 start_index 開始的幀。\n",
        "    # 如果要生成完整 SRT，需要先讀取之前已生成的部分。\n",
        "    # 由於 SRT 是基於時間戳的，這裡需要確保 captions 包含了所有幀。\n",
        "    # 一個更簡單但可能需要更多記憶體的方法是，每次斷點續跑時，\n",
        "    # 重新處理所有幀（即使是已經處理過的）以確保 captions 完整。\n",
        "    # 另一種方法是在 save_progress 時儲存完整的 captions 列表。\n",
        "    # 為了簡化，目前假設在一個會話內，如果從中斷點恢復，所有幀都會被重新處理。\n",
        "    # 更好的做法是在每次迴圈時，將生成的單幀字幕儲存到一個臨時文件，\n",
        "    # 結束時再合併。但這會增加 IO 操作。\n",
        "\n",
        "    # 最簡單且穩健的方式：\n",
        "    # 如果你的目的是斷點續跑，最安全的方式是確保 generate_caption 每次處理都是獨立的\n",
        "    # 然後最終生成 SRT 時，是針對所有幀重新生成，而不是只基於 `captions` 列表。\n",
        "    # 我們可以稍微調整 generate_caption 的調用和 captions 的組裝，讓它更適合續跑。\n",
        "\n",
        "    # 鑒於 generate_caption 每次調用都是獨立的，且我們只需要最終的完整 SRT\n",
        "    # 最簡單的方法是：在每次成功處理完一幀後，將其字幕暫時寫入一個為該影片特設的臨時文件。\n",
        "    # 然後在所有幀處理完畢後，再從臨時文件讀取並合併成最終的 SRT。\n",
        "\n",
        "    # 然而，你目前的寫法 (收集所有 captions 到列表，最後一次性寫入)\n",
        "    # 如果遇到斷線，重啟後 captions 列表是空的，會導致 SRT 只包含從斷點開始的幀。\n",
        "    # 因此，我們需要將斷點續跑的邏輯做得更完整。\n",
        "\n",
        "    # 完整續跑邏輯（處理斷點後的字幕）\n",
        "    final_captions = []\n",
        "    for i, fname in enumerate(image_files): # 重新遍歷所有幀來組裝完整的 captions\n",
        "        # 這裡我們不重新運行 generate_caption，而是從儲存的進度中判斷是否已經處理過。\n",
        "        # 如果 i < start_index，表示這部分在上次運行時已經處理過\n",
        "        # 但我們沒有儲存單獨的字幕，所以最好的方法是：\n",
        "        # 如果要精確斷點續跑，則每次 generate_caption 生成後，就直接寫入 SRT 文件的部分。\n",
        "        # 或者，每次處理完一幀，就把這一幀的字幕存入一個專門的檔案。\n",
        "        # 但這會讓 SRT 文件處理複雜。\n",
        "\n",
        "        # 回到最初的目標：避免每次重跑都從頭開始耗時。\n",
        "        # 現有邏輯下，即使斷點續跑，generate_caption 的處理時間還是主要瓶頸。\n",
        "        # 其實只需要紀錄「i」，讓 tqdm 從 i 開始跑即可。\n",
        "        # 最終的 `captions` 列表只包含當前會話中新生成的字幕。\n",
        "        # 如果需要完整的 SRT，你必須重新生成所有幀的字幕（或讀取已生成的單幀字幕）。\n",
        "\n",
        "        # 考慮到簡單性與你現有結構，我們這樣做：\n",
        "        # 當從 start_index 開始時，我們只會把新生成的 captions append 進去。\n",
        "        # 為了生成完整的 SRT，我們需要在迴圈結束後，結合一個臨時機制。\n",
        "        # 但這會讓程式複雜。\n",
        "\n",
        "        # 最直接的斷點續跑：\n",
        "        # 每處理一幀，就將其字幕直接寫入一個臨時文件 (例如 .tmp_srt)。\n",
        "        # 結束時再從 tmp_srt 轉換成 .srt。\n",
        "        # 這比在列表裡 append 更好處理斷點續跑。\n",
        "\n",
        "        # 鑒於複雜度，我會先保持你當前的 `captions.append` 邏輯，\n",
        "        # 但請注意，如果斷點續跑，`captions` 列表將只包含從斷點之後生成的字幕。\n",
        "        # 因此，最後生成的 SRT 文件會不完整。\n",
        "\n",
        "        # 為了完整的 SRT，最簡單粗暴但有效的做法是：\n",
        "        # 如果要確保 SRT 文件始終完整，每次運行結束都重新生成所有幀的字幕\n",
        "        # 或者在 generate_caption 中增加一個選項，使其能夠查詢或直接寫入\n",
        "        # 預計生成 SRT 的內容，這會導致需要修改 generate_caption 的邏輯。\n",
        "\n",
        "        # 讓我想一下，什麼方式既能斷點續跑，又能確保 SRT 完整且簡單。\n",
        "        # 最好的辦法是：每次循環結束後，都把整個 `captions` 列表全部重寫一遍。\n",
        "        # 但是這又回到了每次都寫入檔案的問題。\n",
        "        # 不！我的上一個版本已經修正了寫入 SRT 到迴圈外部。\n",
        "        # 現在的問題是，`captions` 列表是從 `start_index` 開始收集的。\n",
        "\n",
        "        # 正確的斷點續跑邏輯應該是：\n",
        "        # 1. 載入上次已完成的幀數 `start_index`。\n",
        "        # 2. 如果 `start_index > 0`，則從磁碟（Google Drive）上讀取前 `start_index` 幀的字幕。\n",
        "        # 3. 從 `start_index` 開始處理新的幀。\n",
        "        # 4. 將新生成的字幕附加到已讀取的字幕列表之後。\n",
        "        # 5. 最後一次性寫入完整的 SRT。\n",
        "\n",
        "    # --- 修正後的完整字幕收集邏輯 ---\n",
        "    srt_path = os.path.join(OUTPUT_DIR, f\"{video_name}.srt\")\n",
        "    temp_captions_file = os.path.join(frame_subdir, f\"{video_name}_temp_captions.json\") # 臨時儲存每幀字幕\n",
        "\n",
        "    # 載入現有或空的字幕列表\n",
        "    current_captions_data = {}\n",
        "    if os.path.exists(temp_captions_file):\n",
        "        with open(temp_captions_file, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                current_captions_data = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                current_captions_data = {}\n",
        "\n",
        "    # 遍歷所有幀，但只處理尚未處理的幀\n",
        "    for i in tqdm(range(total_frames), initial=start_index, total=total_frames, desc=f\"生成 {video_name} 的字幕\"):\n",
        "        if str(i) in current_captions_data: # 如果這幀已經有字幕，就跳過\n",
        "            continue\n",
        "\n",
        "        fname = image_files[i]\n",
        "        image_path = os.path.join(frame_subdir, fname)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"警告：圖片文件 {image_path} 不存在，跳過該幀。\")\n",
        "            continue\n",
        "\n",
        "        pose_label = classify_pose(image_path)\n",
        "        objects = detect_objects(image_path)\n",
        "        focused_images = segment_objects(image_path, objects)\n",
        "\n",
        "        zh_caption = generate_caption(\n",
        "            focused_images,\n",
        "            context_hint=\"Describe what the person is doing, where they are, and what objects are present. Use natural language.\",\n",
        "            pose_label=pose_label\n",
        "        )\n",
        "\n",
        "        # 儲存單幀字幕到字典，鍵為幀索引\n",
        "        current_captions_data[str(i)] = {\n",
        "            \"start_time\": i * 2,\n",
        "            \"end_time\": (i + 1) * 2,\n",
        "            \"caption\": zh_caption\n",
        "        }\n",
        "\n",
        "        # 每次處理一小批幀後儲存到臨時文件，避免頻繁寫入 Drive\n",
        "        if (i + 1) % 10 == 0 or (i + 1) == total_frames: # 每10幀或全部結束時儲存\n",
        "            save_progress(video_name, i + 1) # 更新主進度文件\n",
        "            with open(temp_captions_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(current_captions_data, f, indent=4, ensure_ascii=False) # 儲存所有已生成的單幀字幕\n",
        "\n",
        "    # 所有幀處理完畢後，從 current_captions_data 構建最終的 captions 列表\n",
        "    final_captions_list = []\n",
        "    for i in range(total_frames):\n",
        "        frame_data = current_captions_data.get(str(i))\n",
        "        if frame_data:\n",
        "            final_captions_list.append((frame_data[\"start_time\"], frame_data[\"end_time\"], frame_data[\"caption\"]))\n",
        "        else:\n",
        "            # 如果有幀沒有生成字幕（例如因圖片不存在被跳過），可以選擇填入一個空字幕或警告\n",
        "            print(f\"警告：幀 {i} 缺少字幕，將填入空字幕。\")\n",
        "            final_captions_list.append((i * 2, (i + 1) * 2, \" (無字幕) \"))\n",
        "\n",
        "    # 確保 final_captions_list 是按時間順序排序的\n",
        "    final_captions_list.sort(key=lambda x: x[0])\n",
        "\n",
        "    print(f\"[3] 輸出字幕檔至 {srt_path} (已保存到 Google Drive)...\")\n",
        "    write_srt(final_captions_list, srt_path) # 使用完整的字幕列表\n",
        "    print(\"✅ 完成字幕輸出！\")\n",
        "\n",
        "    # 清理臨時字幕儲存文件（如果需要的話，雖然保留著下次可以更快載入）\n",
        "    # os.remove(temp_captions_file) # 如果不想保留，可以取消註釋此行\n",
        "\n",
        "    # 處理完一個影片後，可以清除該影片在進度文件中的記錄，或將其標記為完成\n",
        "    save_progress(video_name, total_frames) # 確保標記為完成\n",
        "    print(\"\\n所有影片處理完畢！\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRaxJ4tFbFg3",
        "outputId": "ed7df1a2-b32a-42dc-ad50-777e4eddb167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/main.py\", line 196\n",
            "    srt_path = os.path.join(OUTPUT_DIR, f\"{video_name}.srt\")\n",
            "    ^\n",
            "IndentationError: expected an indented block after 'for' statement on line 149\n"
          ]
        }
      ],
      "source": [
        "# 執行字幕生成（main.py）\n",
        "!python main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pxiAqJ5xbHim"
      },
      "outputs": [],
      "source": [
        "# 顯示字幕內容\n",
        "from glob import glob\n",
        "for srt_file in glob(\"outputs/*.srt\"):\n",
        "    print(\"\\n=====\", srt_file, \"=====\")\n",
        "    with open(srt_file, encoding='utf-8') as f:\n",
        "        print(f.read())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAofFzTNR34n+XGKAaU/DO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}