{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lurking92/hw_sqj1/blob/main/%E3%80%8CBLIP2_%E9%96%8B%E7%99%BC%E3%80%8D_%E6%B8%AC%E8%A9%A6%E5%96%AE%E4%B8%80%E5%BD%B1%E7%89%87%EF%BC%8C%E8%AB%8B%E6%94%B9%E5%BD%B1%E7%89%87%E5%90%8D%E7%A8%B1(%E6%9C%89%E6%9B%B4%E6%96%B0%EF%BC%8C%E8%AB%8B%E6%8A%8A%E5%85%B6%E4%BB%96%E6%8F%9B%E6%8E%89).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0EoNXzedJO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf3c8c7-3165-4080-defc-19a1f33d927f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "opencc-python-reimplemented\n",
        "torch>=2.0.0\n",
        "transformers>=4.37.0\n",
        "accelerate>=0.24.1\n",
        "Pillow>=10.0.0\n",
        "tqdm\n",
        "opencv-python\n",
        "ffmpeg-python\n",
        "sentencepiece\n",
        "sacremoses\n",
        "mediapipe\n",
        "scipy\n",
        "numpy\n",
        "Pillow\n",
        "einops\n",
        "segment_anything\n",
        "requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls7BvNSSZ7oS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b15f01a-cbe3-4ebf-8fda-5cca71a2f0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencc-python-reimplemented in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.53.1)\n",
            "Requirement already satisfied: accelerate>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.8.1)\n",
            "Requirement already satisfied: Pillow>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.11.0.86)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.1.1)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.10.21)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.15.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.26.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.8.1)\n",
            "Requirement already satisfied: segment_anything in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 3)) (0.33.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 3)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->-r requirements.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.24.1->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python->-r requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses->-r requirements.txt (line 10)) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses->-r requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe->-r requirements.txt (line 11)) (0.5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 17)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 17)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 17)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->-r requirements.txt (line 17)) (2025.7.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.37.0->-r requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 11)) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe->-r requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe->-r requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 11)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 11)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 11)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe->-r requirements.txt (line 11)) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->-r requirements.txt (line 11)) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe->-r requirements.txt (line 11)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# 安裝依賴\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa36GU9Pa-1Q"
      },
      "outputs": [],
      "source": [
        "# 建立資料夾\n",
        "!mkdir -p data/videos data/frames outputs scripts\n",
        "!mkdir -p data/videos data/frames outputs scripts models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifKMCSylUzx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133df515-0066-4acb-da38-eb00354fa3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MediaPipe 姿態偵測模型已存在，跳過下載。\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "model_url = \"https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task\"\n",
        "model_path = \"models/pose_landmarker_heavy.task\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"下載 MediaPipe 姿態偵測模型到 {model_path}...\")\n",
        "    try:\n",
        "        r = requests.get(model_url, allow_redirects=True)\n",
        "        r.raise_for_status() # 檢查請求是否成功 (200 OK)\n",
        "        with open(model_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        print(\"MediaPipe 姿態偵測模型下載完成。\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"下載模型時發生錯誤: {e}\")\n",
        "        print(\"請檢查您的網路連線或稍後再試。\")\n",
        "else:\n",
        "    print(\"MediaPipe 姿態偵測模型已存在，跳過下載。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Ux7v8_zc3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d54b37-14af-48f5-c334-d54f67629300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GLAFj2NbAcu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "27499ec2-4a8b-4a14-8228-92aef29656e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3f43f51c-b16f-41c3-a7d6-feb2bc486803\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3f43f51c-b16f-41c3-a7d6-feb2bc486803\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving my_test_video.mp4 to my_test_video.mp4\n"
          ]
        }
      ],
      "source": [
        "# 上傳影片（可選）\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import shutil\n",
        "import os\n",
        "for fname in uploaded.keys():\n",
        "    os.makedirs(\"data/videos\", exist_ok=True)\n",
        "    shutil.move(fname, f\"data/videos/{fname}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDREcXj-lggV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d066de76-162f-4626-9568-fd865189a77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/write_srt.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/write_srt.py\n",
        "def seconds_to_srt_time(seconds):\n",
        "  hrs = seconds // 3600\n",
        "  mins = (seconds % 3600) // 60\n",
        "  secs = seconds % 60\n",
        "  ms = int((seconds - int(seconds)) * 1000)\n",
        "  return f\"{int(hrs):02}:{int(mins):02}:{int(secs):02},{ms:03}\"\n",
        "\n",
        "def write_srt(captions, output_path):\n",
        "  with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for i, (start, end, text) in enumerate(captions, start=1):\n",
        "      f.write(f\"{i}\\n\")\n",
        "      f.write(f\"{seconds_to_srt_time(start)} --> {seconds_to_srt_time(end)}\\n\")\n",
        "      f.write(f\"{text}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xdlRsi1bCAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47252b3-3f93-4c9f-9ed7-c08dfcd76c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/extract_frames.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/extract_frames.py\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def extract_frames(video_path, output_dir, interval_sec=2):\n",
        "    os.makedirs(output_dir, exist_ok=True) # 確保輸出資料夾存在\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\", video_path,\n",
        "        \"-vf\", f\"fps=1/{interval_sec}\",\n",
        "        os.path.join(output_dir, \"frame_%03d.jpg\") # 這裡會直接寫入 output_dir\n",
        "    ]\n",
        "    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwnZHU_ZcAXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69ca98c-d806-4d53-b148-af7c41adddcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/detect_objects.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/detect_objects.py\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from PIL import Image\n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "from huggingface_hub import login # 保留 login，因為模型可能仍然需要，但它不會影響 CPU 運行\n",
        "\n",
        "# 全局變數，用於載入模型一次\n",
        "processor_gd = None\n",
        "model_gd = None\n",
        "\n",
        "# 使用最小的 Grounding DINO 模型\n",
        "GROUNDING_DINO_MODEL_NAME = \"IDEA-Research/grounding-dino-tiny\"\n",
        "\n",
        "def load_grounding_dino_model():\n",
        "    \"\"\"載入 Grounding DINO 模型。\"\"\"\n",
        "    global processor_gd, model_gd\n",
        "    if processor_gd is None or model_gd is None:\n",
        "        print(f\"載入 Grounding DINO 模型: {GROUNDING_DINO_MODEL_NAME}...\")\n",
        "\n",
        "        try:\n",
        "            # 即使在 CPU 上運行，如果模型是門控的，Hugging Face token 仍然可能需要\n",
        "            login(new_session=False)\n",
        "            print(\"已檢測到 Hugging Face token (如果需要)。\")\n",
        "        except Exception:\n",
        "            print(\"未檢測到 Hugging Face token 或登入失敗，但對於此模型可能不需要。\")\n",
        "\n",
        "        processor_gd = AutoProcessor.from_pretrained(GROUNDING_DINO_MODEL_NAME)\n",
        "        model_gd = AutoModelForZeroShotObjectDetection.from_pretrained(GROUNDING_DINO_MODEL_NAME)\n",
        "\n",
        "        # *** 恢復為自動判斷 GPU/CPU ***\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # <--- 這裡改回來\n",
        "        model_gd.to(device)\n",
        "        print(\"Grounding DINO 模型載入完成。\")\n",
        "        print(f\"Grounding DINO 模型運行設備: {next(model_gd.parameters()).device}\")\n",
        "\n",
        "\n",
        "def detect_objects(image_path, text_prompt, box_threshold=0.3, text_threshold=0.25):\n",
        "    \"\"\"\n",
        "    使用 Grounding DINO 偵測圖片中的物體。\n",
        "\n",
        "    Args:\n",
        "        image_path (str): 圖片檔案的路徑。\n",
        "        text_prompt (str): 用於偵測的文字提示 (例如: \"person. dog. chair.\").\n",
        "        box_threshold (float): 邊界框的置信度閾值。\n",
        "        text_threshold (float): 文字提示與偵測到的物體相關性的閾值。\n",
        "\n",
        "    Returns:\n",
        "        tuple: 包含偵測到的物體標籤列表和對應的邊界框列表。\n",
        "               例如: (['person', 'chair'], [[x1,y1,x2,y2], [x1,y1,x2,y2]])\n",
        "    \"\"\"\n",
        "    load_grounding_dino_model() # 確保模型已載入\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    inputs = processor_gd(images=image, text=text_prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # *** 恢復為將輸入資料移至模型所在的設備 ***\n",
        "    inputs = {k: v.to(model_gd.device) for k, v in inputs.items()} # <--- 這裡改回來\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_gd(**inputs)\n",
        "\n",
        "    results = processor_gd.post_process_grounded_object_detection(\n",
        "        outputs,\n",
        "        threshold=box_threshold,\n",
        "        target_sizes=[image.size[::-1]]\n",
        "    )[0]\n",
        "\n",
        "    detected_labels = []\n",
        "    detected_boxes = []\n",
        "\n",
        "    boxes = results[\"boxes\"].tolist()\n",
        "    scores = results[\"scores\"].tolist()\n",
        "    text_labels = results[\"text_labels\"]\n",
        "\n",
        "    for score, label_text, box in zip(scores, text_labels, boxes):\n",
        "        if score > text_threshold:\n",
        "            detected_labels.append(label_text)\n",
        "            detected_boxes.append(box)\n",
        "\n",
        "    return detected_labels, detected_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YFFYmelcCgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4229de-28a9-4b8a-de0f-9e6fd921c14f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/segment_objects.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/segment_objects.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from PIL import Image\n",
        "import os\n",
        "import requests # 導入 requests 函式庫\n",
        "from tqdm import tqdm # 導入 tqdm 顯示下載進度\n",
        "\n",
        "# 全局變數，用於載入模型一次\n",
        "sam_predictor = None\n",
        "SAM_CHECKPOINT_PATH = \"/content/sam_vit_h_4b8939.pth\" # SAM 模型權重路徑\n",
        "SAM_CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\" # SAM 模型下載 URL\n",
        "\n",
        "def download_sam_checkpoint():\n",
        "    \"\"\"下載 SAM 模型權重（如果不存在）。\"\"\"\n",
        "    if not os.path.exists(SAM_CHECKPOINT_PATH):\n",
        "        print(f\"下載 SAM 模型權重到 {SAM_CHECKPOINT_PATH}...\")\n",
        "        try:\n",
        "            # 使用 requests 函式庫下載檔案\n",
        "            response = requests.get(SAM_CHECKPOINT_URL, stream=True)\n",
        "            response.raise_for_status() # 檢查是否有 HTTP 錯誤\n",
        "            total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
        "            block_size = 1024 # 1 KB\n",
        "\n",
        "            progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "            with open(SAM_CHECKPOINT_PATH, 'wb') as file:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(data))\n",
        "                    file.write(data)\n",
        "            progress_bar.close()\n",
        "\n",
        "            if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "                print(\"錯誤：下載可能不完整！\")\n",
        "            else:\n",
        "                print(\"SAM 模型權重下載完成。\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"下載 SAM 模型時發生錯誤: {e}\")\n",
        "            print(\"請檢查網絡連接或嘗試手動下載檔案並上傳到 /content/\")\n",
        "            exit(1) # 下載失敗則終止程式\n",
        "\n",
        "def load_sam_model():\n",
        "    \"\"\"載入 Segment Anything Model (SAM) 預測器。\"\"\"\n",
        "    global sam_predictor\n",
        "    if sam_predictor is None:\n",
        "        download_sam_checkpoint() # 確保權重已下載\n",
        "        print(\"載入 Segment Anything Model (SAM)...\")\n",
        "        # 確保模型檢查點檔案存在\n",
        "        if not os.path.exists(SAM_CHECKPOINT_PATH):\n",
        "            print(f\"錯誤：SAM 模型檢查點檔案 {SAM_CHECKPOINT_PATH} 不存在。\")\n",
        "            exit(1) # 檔案不存在則終止程式\n",
        "\n",
        "        sam_model = sam_model_registry[\"vit_h\"](checkpoint=SAM_CHECKPOINT_PATH)\n",
        "        if torch.cuda.is_available():\n",
        "            sam_model.to(device=\"cuda\")\n",
        "        sam_predictor = SamPredictor(sam_model)\n",
        "        print(\"SAM 模型載入完成。\")\n",
        "\n",
        "def segment_objects(image_path, boxes):\n",
        "    \"\"\"\n",
        "    使用 Segment Anything Model (SAM) 對圖片中的物體進行分割。\n",
        "\n",
        "    Args:\n",
        "        image_path (str): 圖片檔案的路徑。\n",
        "        boxes (list): Grounding DINO 偵測到的物體的邊界框列表，格式為 [x1, y1, x2, y2]。\n",
        "\n",
        "    Returns:\n",
        "        list: 包含每個分割物件的 PIL.Image 物件列表 (mask) 或原始圖像。\n",
        "              為了簡化，目前回傳原始圖像與遮罩的組合。\n",
        "    \"\"\"\n",
        "    if not boxes: # 如果沒有偵測到物體，則不需要分割\n",
        "        return []\n",
        "\n",
        "    load_sam_model() # 確保模型已載入\n",
        "\n",
        "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
        "    image_np = np.array(image_pil) # 將 PIL 圖片轉換為 NumPy 陣列\n",
        "\n",
        "    sam_predictor.set_image(image_np)\n",
        "\n",
        "    # 確保 input_boxes 是 torch.Tensor 且在正確的 device 上\n",
        "    input_boxes = torch.tensor(boxes, device=sam_predictor.device)\n",
        "\n",
        "    # 進行預測\n",
        "    # 這裡可能需要處理多個邊界框，predict 接受 N, 4 的 tensor\n",
        "    # masks, scores, logits = sam_predictor.predict(\n",
        "    #     point_coords=None,\n",
        "    #     point_labels=None,\n",
        "    #     box=input_boxes,\n",
        "    #     multimask_output=False,\n",
        "    # )\n",
        "    # for boxes, (masks, iou_pred, low_res_logits) in zip(input_boxes, sam_predictor.predict_boxes(input_boxes)):\n",
        "    # SAM's predict method takes a batch of boxes\n",
        "    # Ref: https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/predictor.py#L90\n",
        "\n",
        "    # 批次處理boxes，如果input_boxes有多個，predict可以直接處理\n",
        "    masks, _, _ = sam_predictor.predict_torch(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        boxes=input_boxes, # 這裡使用 boxes 參數\n",
        "        multimask_output=False,\n",
        "    )\n",
        "    # masks 的形狀會是 (num_boxes, 1, H, W)\n",
        "\n",
        "\n",
        "    segmented_images = []\n",
        "    # masks 是一個張量，其形狀為 (N, 1, H, W)，N 是偵測到的物體數量\n",
        "    for mask_tensor in masks:\n",
        "        # 將布林遮罩轉換為 uint8\n",
        "        mask_uint8 = mask_tensor.squeeze().cpu().numpy().astype(np.uint8) * 255\n",
        "\n",
        "        # 創建一個只包含被遮罩區域的圖像\n",
        "        masked_image_np = np.zeros_like(image_np)\n",
        "        # 應用遮罩\n",
        "        masked_image_np[mask_tensor.squeeze().cpu().numpy()] = image_np[mask_tensor.squeeze().cpu().numpy()]\n",
        "\n",
        "        segmented_images.append(Image.fromarray(masked_image_np))\n",
        "\n",
        "    return segmented_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIkn_9PEZYFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d65628-602d-4591-9930-43f533ac3c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/analyze_pose.py\n"
          ]
        }
      ],
      "source": [
        "# scripts/analyze_pose.py\n",
        "%%writefile scripts/analyze_pose.py\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# 全局變數，用於載入模型一次\n",
        "pose_detector = None\n",
        "\n",
        "def load_pose_detector():\n",
        "    \"\"\"載入 MediaPipe 姿態偵測模型 (使用 MediaPipe Tasks API)。\"\"\"\n",
        "    global pose_detector\n",
        "    if pose_detector is None:\n",
        "        print(\"載入 MediaPipe 姿態偵測模型: models/pose_landmarker_heavy.task...\")\n",
        "        # 確保模型檔案路徑正確\n",
        "        base_options = python.BaseOptions(model_asset_path='models/pose_landmarker_heavy.task')\n",
        "        options = vision.PoseLandmarkerOptions(base_options=base_options, output_segmentation_masks=False)\n",
        "        pose_detector = vision.PoseLandmarker.create_from_options(options)\n",
        "        print(\"MediaPipe 姿態偵測模型載入完成。\")\n",
        "\n",
        "def get_pose_label(landmarks):\n",
        "    \"\"\"\n",
        "    根據關鍵點判斷大致的姿態。\n",
        "    這個函數需要根據實際需求細化，目前提供簡單示例。\n",
        "    注意：這裡的判斷邏輯是基於 NormalizedLandmark 的 y 座標 (0.0 到 1.0)。\n",
        "    \"\"\"\n",
        "    if not landmarks:\n",
        "        return \"無法辨識\"\n",
        "\n",
        "    # 獲取部分關鍵點的 Y 座標\n",
        "    # MediaPipe 關鍵點索引參考: https://developers.google.com/mediapipe/solutions/vision/pose_landmarker\n",
        "    # 例如：NOSE (0), LEFT_KNEE (25), RIGHT_KNEE (26), LEFT_ANKLE (27), RIGHT_ANKLE (28)\n",
        "\n",
        "    # 使用 try-except 確保關鍵點存在\n",
        "    try:\n",
        "        nose_y = landmarks[mp.solutions.pose.PoseLandmark.NOSE.value].y\n",
        "        left_knee_y = landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].y\n",
        "        right_knee_y = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].y\n",
        "        left_ankle_y = landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].y\n",
        "        right_ankle_y = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].y\n",
        "    except IndexError:\n",
        "        # 如果關鍵點列表不完整，無法進行判斷\n",
        "        return \"無法辨識\"\n",
        "\n",
        "    avg_knee_y = (left_knee_y + right_knee_y) / 2\n",
        "    avg_ankle_y = (left_ankle_y + right_ankle_y) / 2\n",
        "\n",
        "    # 計算軀幹的垂直跨度 (從鼻子到腳踝)\n",
        "    vertical_span = avg_ankle_y - nose_y\n",
        "\n",
        "    # 基於關鍵點的Y座標進行姿態判斷（這裡的閾值需要根據實際數據微調）\n",
        "    # Y軸向下為正，值越大表示位置越低\n",
        "\n",
        "    # 跌倒：身體非常低，垂直跨度非常小\n",
        "    # 假設圖像高度為1，如果鼻子y座標很高（接近底部）且垂直跨度小\n",
        "    if nose_y > 0.65 and vertical_span < 0.2: # 鼻子靠近底部，且身體被壓縮\n",
        "        return \"跌倒\"\n",
        "    # 坐下：鼻子和膝蓋的Y座標差異小（膝蓋相對較高），且身體總體高度較低\n",
        "    elif (nose_y - avg_knee_y) < 0.1 and vertical_span < 0.5: # 膝蓋靠近鼻子，且身體不完全伸展\n",
        "        return \"坐下\"\n",
        "    # 走路：軀幹垂直跨度在一定範圍內，表示處於中間姿態\n",
        "    elif 0.5 <= vertical_span <= 0.8:\n",
        "        return \"走路\"\n",
        "    # 站立：軀幹垂直跨度較大，身體伸展\n",
        "    elif vertical_span > 0.8:\n",
        "        return \"站立\"\n",
        "    else:\n",
        "        return \"活動\" # 預設為活動，或表示無法精確分類\n",
        "\n",
        "\n",
        "def analyze_pose(image_path): # *** 保持這個函數名稱為 analyze_pose ***\n",
        "    \"\"\"\n",
        "    分析圖片中的人體姿態並返回一個標籤。\n",
        "    \"\"\"\n",
        "    load_pose_detector() # 確保模型已載入\n",
        "\n",
        "    # 載入圖片並轉換為 MediaPipe 圖像格式\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.asarray(image))\n",
        "\n",
        "    # 執行姿態偵測\n",
        "    detection_result = pose_detector.detect(mp_image)\n",
        "\n",
        "    # 提取關鍵點\n",
        "    if detection_result.pose_landmarks:\n",
        "        # MediaPipe 可能偵測到多個人，這裡簡化處理，只取第一個人的姿態\n",
        "        landmarks = detection_result.pose_landmarks[0] # 獲取第一個偵測到的人的關鍵點列表\n",
        "\n",
        "        # 獲取姿態標籤\n",
        "        pose_label = get_pose_label(landmarks)\n",
        "        return pose_label\n",
        "    else:\n",
        "        return \"無法辨識\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZWYVFMYlZvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82641fa0-9f6d-47c1-849a-535f29f6b650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/utils.py\n",
        "import subprocess\n",
        "import json\n",
        "import datetime # 需要 datetime 模組來處理時間\n",
        "from datetime import timedelta\n",
        "\n",
        "def get_video_duration_ffmpeg(video_path):\n",
        "    \"\"\"\n",
        "    使用 ffmpeg 獲取影片的時長 (秒)。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        command = [\n",
        "            'ffprobe',\n",
        "            '-v', 'error',\n",
        "            '-show_entries', 'format=duration',\n",
        "            '-of', 'default=noprint_wrappers=1:nokey=1',\n",
        "            video_path\n",
        "        ]\n",
        "        duration_str = subprocess.check_output(command, stderr=subprocess.STDOUT).decode().strip()\n",
        "        return float(duration_str)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting video duration with ffmpeg: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_txt(captions_list, output_path):\n",
        "    \"\"\"\n",
        "    將字幕列表保存為純文字檔案 (.txt)。\n",
        "    captions_list 預期是一個只有字幕文字的列表。\n",
        "    \"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for caption in captions_list:\n",
        "            f.write(caption + '\\n')\n",
        "    print(f\"字幕已儲存為純文字檔案到 {output_path}\")\n",
        "\n",
        "# 注意：save_transcription_to_srt 函數已移除，因為 main.py 不再需要它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUxwvDaJcExA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db76b9d4-6cc8-4413-9cfd-1d6efacead6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/generate_caption.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/generate_caption.py\n",
        "# scripts/generate_caption.py\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, pipeline\n",
        "from PIL import Image\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "from opencc import OpenCC\n",
        "\n",
        "# 模型設定\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "model = model.to(device)\n",
        "print(f\"BLIP2 模型運行設備: {next(model.parameters()).device}\")\n",
        "\n",
        "# 翻譯和轉換\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\", src_lang=\"en\", tgt_lang=\"zh\", device=0 if torch.cuda.is_available() else -1)\n",
        "cc = OpenCC('s2twp')\n",
        "\n",
        "# 無效描述的過濾清單 (新增了更多通用詞和網址相關詞)\n",
        "INVALID_DESCRIPTIONS = [\n",
        "    \"gta san andreas\", \"screenshot\", \"jpg\", \"png\", \"image\", \"photo\", \"picture\",\n",
        "    \"describe\", \"what\", \"you\", \"see\", \"in\", \"the\", \"image\", \"this\", \"is\", \"a\",\n",
        "    \"man\", \"woman\", \"person\", \"standing\", \"sitting\", \"walking\", \"running\",\n",
        "    \"game\", \"video\", \"frame\", \"scene\", \"view\", \"show\", \"display\", \"appear\",\n",
        "    \"can\", \"will\", \"be\", \"given\", \"option\", \"mind\", \"words\", \"few\", \"left\",\n",
        "    \"right\", \"front\", \"back\", \"name\", \"set\", \"have\", \"to\", \"your\",\n",
        "    \"youtube\", \"youtu\", \"com\", \"http\", \"https\", \"www\", \"googleusercontent\" # 新增網址相關詞\n",
        "]\n",
        "\n",
        "def is_valid_description(text):\n",
        "    \"\"\"檢查描述是否有效\"\"\"\n",
        "    if not text or len(text.strip()) < 5:\n",
        "        return False\n",
        "\n",
        "    # 轉換為小寫並移除標點符號\n",
        "    clean_text_lower = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    words = clean_text_lower.split()\n",
        "\n",
        "    # 檢查是否包含太多無效詞彙\n",
        "    invalid_count = sum(1 for word in words if word in INVALID_DESCRIPTIONS)\n",
        "    if len(words) > 0 and invalid_count / len(words) > 0.6:  # 如果超過60%是無效詞彙\n",
        "        return False\n",
        "\n",
        "    # 檢查是否是常見的無效模式 (新增網址模式)\n",
        "    invalid_patterns = [\n",
        "        r\"^describe.*image\", r\"^what.*see\", r\"^you.*describe\",\n",
        "        r\"^this.*screenshot\", r\"^gta.*andreas\", r\"^the.*game\",\n",
        "        r\"^a\\s+(man|woman|person)\\s+is\\s+$\", r\"^.*standing.*front.*$\",\n",
        "        r\"https?://(?:www\\.)?(?:youtube\\.com|youtu\\.be|googleusercontent\\.com/youtube\\.com)/?\\S*\", # 匹配各種 YouTube 網址\n",
        "        r\"youtube\", r\"youtu\", r\"com\", r\"http\", r\"https\", r\"www\", r\"googleusercontent\" # 再次檢查關鍵詞\n",
        "    ]\n",
        "\n",
        "    for pattern in invalid_patterns:\n",
        "        if re.search(pattern, clean_text_lower): # 對清理過的小寫文本進行匹配\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"清理生成的文字，強化去重複和網址移除\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # 移除常見的無用前綴和後綴 (包含中文和英文)\n",
        "    text = re.sub(r\"^(this is |there is |the image shows |image shows |a man |a woman |a person |screenshot of |image of )\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"^(這是|影像顯示|圖片顯示|畫面顯示|一個人|一個男人|一個女人|螢幕截圖)\", \"\", text)\n",
        "\n",
        "    # 移除無意義的結尾\n",
        "    text = re.sub(r\"(\\s+in\\s+the\\s+image|\\s+of\\s+the\\s+game|\\s+in\\s+gta|\\s+screenshot)$\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 移除各種 YouTube 網址和相關關鍵字\n",
        "    text = re.sub(r\"https?://(?:www\\.)?(?:youtube\\.com|youtu\\.be|googleusercontent\\.com/youtube\\.com)/?\\S*\", \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"(youtube|youtu|com|http|https|www|googleusercontent)\\s*\", \"\", text, flags=re_IGNORECASE) # 移除關鍵詞\n",
        "\n",
        "    # 移除重複的詞語：例如 \"椅子 椅子\" -> \"椅子\", \"表格表格表格\" -> \"表格\"\n",
        "    # 這段正則表達式會匹配連續重複的中文詞或英文單詞，並替換為單個\n",
        "    text = re.sub(r\"([\\u4e00-\\u9fa5a-zA-Z0-9]+)\\s*\\1+\", r\"\\1\", text) # 處理 \"詞 詞\"\n",
        "    text = re.sub(r\"([\\u4e00-\\u9fa5a-zA-Z0-9]{2,})\\1+\", r\"\\1\", text) # 處理 \"詞詞詞\" (至少兩個字重複)\n",
        "\n",
        "    # 移除多餘的標點和空格\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip(\" .,。，\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def generate_better_caption(image_path, pose_label=None, detected_objects=None):\n",
        "    \"\"\"為單張圖像生成更好的描述\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # 嘗試多種生成策略\n",
        "    best_caption = \"\"\n",
        "    attempts = 0\n",
        "    max_attempts = 3\n",
        "\n",
        "    while attempts < max_attempts and not best_caption:\n",
        "        try:\n",
        "            prompt = \"Describe this image.\" # 統一使用簡潔提示\n",
        "\n",
        "            inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=30,\n",
        "                    min_length=5,\n",
        "                    num_beams=5,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    repetition_penalty=1.2,\n",
        "                    length_penalty=1.0,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "\n",
        "            generated_text = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            # 移除提示詞部分\n",
        "            # 這裡需要更精確地移除輸入的 prompt 部分\n",
        "            # 找到 prompt 在 generated_text 中的位置，然後移除\n",
        "            if prompt in generated_text:\n",
        "                generated_text = generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "            # 檢查生成的文字是否有效\n",
        "            if is_valid_description(generated_text):\n",
        "                best_caption = generated_text\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"生成嘗試 {attempts + 1} 失敗: {e}\")\n",
        "\n",
        "        attempts += 1\n",
        "\n",
        "    # 如果所有嘗試都失敗，使用後備方案\n",
        "    if not best_caption:\n",
        "        if pose_label and pose_label != \"無法辨識\":\n",
        "            best_caption = f\"person {pose_label}\"\n",
        "        elif detected_objects:\n",
        "            # 嘗試從物件中組合一個簡單描述\n",
        "            filtered_objects = [obj for obj in detected_objects if obj.lower() != \"person\"]\n",
        "            if filtered_objects:\n",
        "                best_caption = f\"scene with {', '.join(filtered_objects[:2])}\" # 最多兩個物件\n",
        "            else:\n",
        "                best_caption = \"scene\"\n",
        "        else:\n",
        "            best_caption = \"scene\" # 最簡單的後備\n",
        "\n",
        "    return best_caption\n",
        "\n",
        "def generate_caption(image_paths_batch, pose_labels_batch=None, detected_objects_batch=None, debug_log_path=\"outputs/debug.log\"):\n",
        "    \"\"\"生成圖像描述字幕 - 改進版\"\"\"\n",
        "    debug_log = open(debug_log_path, \"a\", encoding=\"utf-8\")\n",
        "\n",
        "    # 為每張圖像生成描述\n",
        "    raw_english_captions = []\n",
        "    for idx, image_path in enumerate(image_paths_batch):\n",
        "        pose_label = pose_labels_batch[idx] if pose_labels_batch else None\n",
        "        detected_objects = detected_objects_batch[idx] if detected_objects_batch else None\n",
        "\n",
        "        caption = generate_better_caption(image_path, pose_label, detected_objects)\n",
        "        raw_english_captions.append(caption)\n",
        "\n",
        "    print(f\"英文描述: {raw_english_captions}\")\n",
        "\n",
        "    # 翻譯成中文\n",
        "    translated_texts = []\n",
        "    for caption in raw_english_captions:\n",
        "        if caption.strip():\n",
        "            try:\n",
        "                translated = translator(caption)\n",
        "                if isinstance(translated, list) and len(translated) > 0:\n",
        "                    if isinstance(translated[0], dict) and 'translation_text' in translated[0]:\n",
        "                        translated_texts.append(translated[0]['translation_text'])\n",
        "                    else:\n",
        "                        translated_texts.append(str(translated[0]))\n",
        "                else:\n",
        "                    translated_texts.append(\"\")\n",
        "            except Exception as e:\n",
        "                print(f\"翻譯失敗: {e}\")\n",
        "                translated_texts.append(\"\")\n",
        "        else:\n",
        "            translated_texts.append(\"\")\n",
        "\n",
        "    # 生成最終字幕\n",
        "    final_zh_captions = []\n",
        "    for idx, (raw_caption, zh_translation) in enumerate(zip(raw_english_captions, translated_texts)):\n",
        "        # 轉換成繁體中文並清理\n",
        "        zh_trad = cc.convert(zh_translation) if zh_translation else \"\"\n",
        "        clean_caption = clean_text(zh_trad) # 對翻譯後的文字進行清理\n",
        "\n",
        "        # 構建基礎描述\n",
        "        base_description = clean_caption if clean_caption else \"畫面內容\" # 如果清理後為空，使用通用詞\n",
        "\n",
        "        # 添加姿勢信息\n",
        "        pose_info = \"\"\n",
        "        if pose_labels_batch and pose_labels_batch[idx] and pose_labels_batch[idx] != \"無法辨識\":\n",
        "            # 避免與基礎描述重複，例如如果基礎描述已經是「一個人走路」，就不再重複「走路」\n",
        "            if pose_labels_batch[idx] not in base_description:\n",
        "                pose_info = f\"，{pose_labels_batch[idx]}\"\n",
        "            else:\n",
        "                pose_info = f\"（{pose_labels_batch[idx]}）\" # 動作標籤，用括號表示補充\n",
        "\n",
        "        # 添加物件信息\n",
        "        object_info = \"\"\n",
        "        if detected_objects_batch and detected_objects_batch[idx]:\n",
        "            unique_objects = list(set(detected_objects_batch[idx]))\n",
        "            filtered_objects = [obj for obj in unique_objects if obj.lower() != \"person\"]\n",
        "\n",
        "            if filtered_objects:\n",
        "                try:\n",
        "                    # 只翻譯前3個最重要的物件\n",
        "                    objects_to_translate = filtered_objects[:3]\n",
        "                    translated_objects_raw = translator(objects_to_translate)\n",
        "                    zh_objects_cleaned = []\n",
        "\n",
        "                    for obj_trans in translated_objects_raw:\n",
        "                        translated_obj_text = \"\"\n",
        "                        if isinstance(obj_trans, dict) and 'translation_text' in obj_trans:\n",
        "                            translated_obj_text = obj_trans['translation_text']\n",
        "                        elif isinstance(obj_trans, str):\n",
        "                            translated_obj_text = obj_trans\n",
        "                        elif isinstance(obj_trans, list) and len(obj_trans) > 0 and isinstance(obj_trans[0], dict) and 'translation_text' in obj_trans[0]:\n",
        "                            translated_obj_text = obj_trans[0]['translation_text']\n",
        "\n",
        "                        if translated_obj_text:\n",
        "                            # 對單個物件的翻譯結果也進行清理和去重複\n",
        "                            cleaned_obj_text = clean_text(cc.convert(translated_obj_text))\n",
        "                            if cleaned_obj_text and cleaned_obj_text not in zh_objects_cleaned:  # 避免重複添加\n",
        "                                zh_objects_cleaned.append(cleaned_obj_text)\n",
        "\n",
        "                    if zh_objects_cleaned:\n",
        "                        object_info = f\"，可見{', '.join(zh_objects_cleaned)}\"\n",
        "                except Exception as e:\n",
        "                    print(f\"物件翻譯失敗: {e}\")\n",
        "\n",
        "        # 組合最終字幕\n",
        "        final_caption = f\"{base_description}{pose_info}{object_info}\"\n",
        "\n",
        "        # 如果最終字幕仍然包含過多無效詞或過短，則使用更通用的後備\n",
        "        if not is_valid_description(final_caption) or len(final_caption.strip()) < 5:\n",
        "            if pose_labels_batch and pose_labels_batch[idx] and pose_labels_batch[idx] != \"無法辨識\":\n",
        "                final_caption = f\"畫面中一個人正在{pose_labels_batch[idx]}\"\n",
        "            elif detected_objects_batch and detected_objects_batch[idx]:\n",
        "                filtered_objects = [obj for obj in detected_objects_batch[idx] if obj.lower() != \"person\"]\n",
        "                if filtered_objects:\n",
        "                    # 再次嘗試翻譯和清理物件\n",
        "                    try:\n",
        "                        objects_to_translate = filtered_objects[:2]\n",
        "                        translated_objects_raw = translator(objects_to_translate)\n",
        "                        zh_objects_fallback = []\n",
        "                        for obj_trans in translated_objects_raw:\n",
        "                            translated_obj_text = \"\"\n",
        "                            if isinstance(obj_trans, dict) and 'translation_text' in obj_trans:\n",
        "                                translated_obj_text = obj_trans['translation_text']\n",
        "                            elif isinstance(obj_trans, str):\n",
        "                                translated_obj_text = obj_trans\n",
        "                            elif isinstance(obj_trans, list) and len(obj_trans) > 0 and isinstance(obj_trans[0], dict) and 'translation_text' in obj_trans[0]:\n",
        "                                translated_obj_text = obj_trans[0]['translation_text']\n",
        "                            if translated_obj_text:\n",
        "                                cleaned_obj_text = clean_text(cc.convert(translated_obj_text))\n",
        "                                if cleaned_obj_text and cleaned_obj_text not in zh_objects_fallback:\n",
        "                                    zh_objects_fallback.append(cleaned_obj_text)\n",
        "                        if zh_objects_fallback:\n",
        "                            final_caption = f\"畫面中可見{', '.join(zh_objects_fallback)}\"\n",
        "                        else:\n",
        "                            final_caption = \"畫面內容不詳\"\n",
        "                    except Exception:\n",
        "                        final_caption = \"畫面內容不詳\"\n",
        "                else:\n",
        "                    final_caption = \"畫面內容不詳\"\n",
        "            else:\n",
        "                final_caption = \"畫面內容不詳\"\n",
        "\n",
        "\n",
        "        # 記錄調試信息\n",
        "        debug_log.write(f\"圖像: {image_paths_batch[idx]}\\n\")\n",
        "        debug_log.write(f\"  原始英文: {raw_caption}\\n\")\n",
        "        debug_log.write(f\"  中文翻譯: {zh_translation}\\n\")\n",
        "        debug_log.write(f\"  清理後: {clean_caption}\\n\")\n",
        "        debug_log.write(f\"  姿勢標籤: {pose_labels_batch[idx] if pose_labels_batch else 'None'}\\n\")\n",
        "        debug_log.write(f\"  偵測物件: {detected_objects_batch[idx] if detected_objects_batch else 'None'}\\n\")\n",
        "        debug_log.write(f\"  最終結果: {final_caption}\\n\\n\")\n",
        "\n",
        "        final_zh_captions.append(final_caption)\n",
        "\n",
        "    debug_log.close()\n",
        "    return final_zh_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwEUNroBcJfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc2fffa-a948-4a3a-deeb-1478931b93c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm # 引入 tqdm\n",
        "\n",
        "# 導入所有必要的腳本函數\n",
        "from scripts.extract_frames import extract_frames\n",
        "from scripts.detect_objects import detect_objects\n",
        "from scripts.segment_objects import segment_objects # 即使目前未啟用，也需要確保能導入\n",
        "from scripts.analyze_pose import analyze_pose\n",
        "from scripts.generate_caption import generate_caption\n",
        "from scripts.utils import save_txt, get_video_duration_ffmpeg # 只導入 TXT 和 FFmpeg 相關函數\n",
        "\n",
        "# --- 設定變數 ---\n",
        "VIDEO_FILE_NAME = \"my_test_video.mp4\" # 請替換為你的影片檔案名稱\n",
        "INPUT_VIDEO_PATH = f\"/content/drive/MyDrive/{VIDEO_FILE_NAME}\" # 影片在 Google Drive 中的路徑\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab_Subtitles_Output\" # 所有輸出的根目錄\n",
        "FRAMES_DIR = os.path.join(OUTPUT_DIR, \"frames\", os.path.splitext(VIDEO_FILE_NAME)[0]) # 影片幀的輸出目錄\n",
        "TEMP_OBJECTS_DIR = os.path.join(OUTPUT_DIR, \"temp_objects\", os.path.splitext(VIDEO_FILE_NAME)[0]) # 分割物體的臨時目錄\n",
        "# SRT_OUTPUT_PATH 不再需要\n",
        "TXT_OUTPUT_PATH = os.path.join(OUTPUT_DIR, f\"{os.path.splitext(VIDEO_FILE_NAME)[0]}.txt\") # TXT 字幕輸出路徑\n",
        "\n",
        "FRAME_INTERVAL = 2 # 每隔多少秒提取一幀\n",
        "\n",
        "# 每批次處理多少幀 (根據你的 GPU 記憶體調整，越大越快但越耗記憶體)\n",
        "BATCH_SIZE = 8 # 建議值，可以嘗試調整\n",
        "\n",
        "# --- 確保輸出目錄存在 ---\n",
        "os.makedirs(FRAMES_DIR, exist_ok=True)\n",
        "os.makedirs(TEMP_OBJECTS_DIR, exist_ok=True) # 確保物體分割臨時目錄存在\n",
        "\n",
        "# --- 步驟 0: 確認 Google Drive 已掛載 ---\n",
        "print(\"--- 步驟 0: 確認 Google Drive 已掛載 ---\")\n",
        "if os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    print(\"Google Drive 已掛載。\")\n",
        "else:\n",
        "    print(\"警告: Google Drive 未掛載，請確保已運行相關掛載步驟。\")\n",
        "\n",
        "# --- 步驟 1: 提取影片幀 ---\n",
        "print(\"\\n--- 步驟 1: 提取影片幀 ---\")\n",
        "# 檢查是否已存在，如果不存在則提取\n",
        "if not os.path.exists(os.path.join(FRAMES_DIR, \"frame_00001.jpg\")):\n",
        "    print(f\"[1] 正在提取影片 {VIDEO_FILE_NAME} 的圖片幀...\")\n",
        "    extract_frames(INPUT_VIDEO_PATH, FRAMES_DIR, FRAME_INTERVAL)\n",
        "    print(f\"[1] 影片幀已提取到: {FRAMES_DIR}\")\n",
        "else:\n",
        "    print(f\"[1] 影片 {VIDEO_FILE_NAME} 的圖片幀已存在於 Google Drive ({FRAMES_DIR})，跳過提取。\")\n",
        "\n",
        "# --- 步驟 2: 處理每一幀以生成字幕 ---\n",
        "print(f\"\\n--- 步驟 2: 處理 {VIDEO_FILE_NAME} 的每一幀並生成字幕 ---\")\n",
        "\n",
        "# 讀取所有圖片幀的路徑，並按數字順序排序\n",
        "frame_files = sorted([f for f in os.listdir(FRAMES_DIR) if f.endswith('.jpg')])\n",
        "image_paths = [os.path.join(FRAMES_DIR, f) for f in frame_files]\n",
        "\n",
        "# 初始化保存字幕的列表 (只包含字幕文字，因為不再需要SRT的時間戳)\n",
        "all_transcriptions_text_only = []\n",
        "existing_transcriptions_count = 0\n",
        "\n",
        "# 檢查字幕 TXT 檔案是否存在，如果存在則讀取它並跳過已處理的幀\n",
        "if os.path.exists(TXT_OUTPUT_PATH):\n",
        "    print(f\"警告：影片 {VIDEO_FILE_NAME} 的 TXT 檔案已存在，將嘗試從中恢復進度。\")\n",
        "    with open(TXT_OUTPUT_PATH, 'r', encoding='utf-8') as f:\n",
        "        existing_lines = f.readlines()\n",
        "        existing_transcriptions_count = len(existing_lines)\n",
        "        for line in existing_lines:\n",
        "            all_transcriptions_text_only.append(line.strip())\n",
        "\n",
        "    print(f\"已從 TXT 檔案中恢復 {existing_transcriptions_count} 條現有字幕。\")\n",
        "    # 如果已處理的幀數與總幀數相同，則直接完成\n",
        "    if existing_transcriptions_count >= len(image_paths):\n",
        "        print(f\"所有幀都已在 TXT 檔案中處理過。\")\n",
        "        print(\"所有任務完成！你可以在 Google Drive 中找到 TXT 輸出。\") # 統一成功提示\n",
        "        exit() # 退出程式\n",
        "\n",
        "# 從上次中斷的地方繼續處理\n",
        "start_frame_index = existing_transcriptions_count\n",
        "print(f\"將從第 {start_frame_index + 1} 幀開始處理 (共 {len(image_paths)} 幀)。\")\n",
        "\n",
        "# 刪除上次可能殘留的臨時物體分割圖像 (每次運行都清理，避免積累)\n",
        "if os.path.exists(TEMP_OBJECTS_DIR):\n",
        "    shutil.rmtree(TEMP_OBJECTS_DIR)\n",
        "os.makedirs(TEMP_OBJECTS_DIR, exist_ok=True)\n",
        "\n",
        "# 使用 tqdm 顯示進度條\n",
        "with tqdm(total=len(image_paths), initial=start_frame_index, desc=f\"生成 {VIDEO_FILE_NAME} 的字幕\") as pbar:\n",
        "    for i in range(start_frame_index, len(image_paths), BATCH_SIZE):\n",
        "        batch_image_paths = image_paths[i:i + BATCH_SIZE]\n",
        "\n",
        "        # --- 每個批次處理前的時間戳 ---\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Step 2.1: 偵測物體\n",
        "        print(f\"\\n[進度 {pbar.n}/{len(image_paths)}] 正在偵測批次物體 (幀 {i+1} 到 {min(i + BATCH_SIZE, len(image_paths))})...\")\n",
        "        batch_objects = []\n",
        "        for img_path in batch_image_paths:\n",
        "            objects, _ = detect_objects(img_path, text_prompt=\"person, car, animal, road, building, tree, sky, water, table, chair, computer, phone, book, cup, food, clothes, bag, hand, head, body, sitting, standing, walking, running, jumping, eating, drinking, holding, looking, talking, working\")\n",
        "            batch_objects.append(objects)\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 物體偵測完成。\")\n",
        "\n",
        "        # Step 2.2: 分析姿態\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 正在分析批次姿態...\")\n",
        "        batch_pose_labels = []\n",
        "        for img_path in batch_image_paths:\n",
        "            pose_label = analyze_pose(img_path)\n",
        "            batch_pose_labels.append(pose_label)\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 姿態分析完成。\")\n",
        "\n",
        "        # Step 2.3: 生成字幕\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 正在生成批次字幕...\")\n",
        "        batch_captions = generate_caption(batch_image_paths, batch_pose_labels, batch_objects)\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 字幕生成完成。\")\n",
        "\n",
        "        # 將生成的字幕添加到總列表\n",
        "        all_transcriptions_text_only.extend(batch_captions)\n",
        "\n",
        "        # 每個批次處理後的總時間\n",
        "        batch_end_time = time.time()\n",
        "        batch_duration = batch_end_time - batch_start_time\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 批次處理時間: {timedelta(seconds=batch_duration)}\")\n",
        "\n",
        "        # 定期保存進度到 TXT 檔案\n",
        "        save_txt(all_transcriptions_text_only, TXT_OUTPUT_PATH)\n",
        "        print(f\"[進度 {pbar.n}/{len(image_paths)}] 進度已保存到 TXT 檔案: {TXT_OUTPUT_PATH}\")\n",
        "\n",
        "        # 更新 tqdm 進度條\n",
        "        pbar.update(len(batch_image_paths))\n",
        "\n",
        "\n",
        "print(\"\\n[3] 所有幀處理完成。\")\n",
        "\n",
        "# --- 步驟 3: 字幕處理完成，確認 TXT 檔案已保存 ---\n",
        "print(f\"\\n--- 步驟 3: 字幕處理完成，確認 TXT 檔案已保存 ---\")\n",
        "print(f\"最終 TXT 字幕已保存到: {TXT_OUTPUT_PATH}\")\n",
        "\n",
        "print(\"\\n所有任務完成！你可以在 Google Drive 中找到 TXT 輸出。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRaxJ4tFbFg3",
        "outputId": "3c4ac663-085f-4678-a6b5-fb2e8c901afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-13 16:56:16.046302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752425776.236636   37379 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752425776.284848   37379 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-13 16:56:16.697795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Loading checkpoint shards: 100% 2/2 [01:05<00:00, 32.88s/it]\n",
            "BLIP2 模型運行設備: cuda:0\n",
            "Device set to use cuda:0\n",
            "--- 步驟 0: 確認 Google Drive 已掛載 ---\n",
            "Google Drive 已掛載。\n",
            "\n",
            "--- 步驟 1: 提取影片幀 ---\n",
            "[1] 正在提取影片 my_test_video.mp4 的圖片幀...\n",
            "[1] 影片幀已提取到: /content/drive/MyDrive/Colab_Subtitles_Output/frames/my_test_video\n",
            "\n",
            "--- 步驟 2: 處理 my_test_video.mp4 的每一幀並生成字幕 ---\n",
            "將從第 1 幀開始處理 (共 0 幀)。\n",
            "生成 my_test_video.mp4 的字幕: 0it [00:00, ?it/s]\n",
            "\n",
            "[3] 所有幀處理完成。\n",
            "\n",
            "--- 步驟 3: 字幕處理完成，確認 TXT 檔案已保存 ---\n",
            "最終 TXT 字幕已保存到: /content/drive/MyDrive/Colab_Subtitles_Output/my_test_video.txt\n",
            "\n",
            "所有任務完成！你可以在 Google Drive 中找到 TXT 輸出。\n"
          ]
        }
      ],
      "source": [
        "# 執行字幕生成（main.py）\n",
        "!python main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxiAqJ5xbHim"
      },
      "outputs": [],
      "source": [
        "# 顯示字幕內容\n",
        "from glob import glob\n",
        "for srt_file in glob(\"outputs/*.srt\"):\n",
        "    print(\"\\n=====\", srt_file, \"=====\")\n",
        "    with open(srt_file, encoding='utf-8') as f:\n",
        "        print(f.read())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMctXxpGlTG8cde/gAEuLNY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}